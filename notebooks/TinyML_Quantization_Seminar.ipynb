{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6649a0b3",
   "metadata": {},
   "source": [
    "\n",
    "# TinyML Seminar — Quantization Hands‑On (PyTorch)\n",
    "\n",
    "**Facilitator:** Obed Mogaka  \n",
    "\n",
    "**Department Seminar — Mixed Audience (Students & Faculty)**  \n",
    "\n",
    "**Focus:** Quantization (PTQ, QAT, Custom Quantizer)  \n",
    "\n",
    "**Runtime:** ~3 hours (or two 90‑minute sessions)\n",
    "\n",
    "> This notebook is intentionally scaffolded with **explanations and TODOs**. \n",
    "> You will fill in the code live during the session. Keep it lightweight and interactive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f2d2",
   "metadata": {},
   "source": [
    "\n",
    "## Agenda & Learning Outcomes\n",
    "\n",
    "**Modules**\n",
    "1. Visualization — FP32 vs INT* discretization + error metrics  \n",
    "2. DNN Primer — CIFAR‑10, small CNN, MobileNetV2 overview; histograms, size, latency  \n",
    "3. PTQ — PyTorch built‑ins on small CNN & MobileNetV2; compare metrics  \n",
    "4. QAT — Built‑in QAT on small CNN (and/or MobileNetV2)  \n",
    "5. User‑Defined Quantizer — Manual linear quantization + STE for QAT (advanced)\n",
    "\n",
    "**You will be able to:**\n",
    "- Explain quantization intuitively and mathematically.\n",
    "- Run PTQ and QAT in PyTorch and interpret the trade‑offs.\n",
    "- Implement and experiment with a custom quantizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b0bd3",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup & Environment\n",
    "\n",
    "> Keep installs minimal for CPU‑only environments. Pre‑download CIFAR‑10 to avoid bandwidth surprises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc195bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: (optional) installs if running in a fresh environment\n",
    "# %pip install torch torchvision matplotlib numpy --quiet\n",
    "\n",
    "# TODO: standard imports (keep here for participants to run once)\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torchvision\n",
    "# from torchvision import datasets, transforms, models\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import os\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# SEED = 42\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee01e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Module 1 — Quantization Visualization\n",
    "\n",
    "**Goal:** Build intuition with numbers **before** models.  \n",
    "**Concepts:** Discretization, step size (Δ), clipping, rounding, error metrics (MSE, max error).\n",
    "\n",
    "### What to do\n",
    "1. Generate continuous numeric data (e.g., sine wave).\n",
    "2. Quantize to 16‑, 8‑, 4‑, 2‑bit with **uniform** quantization.\n",
    "3. Plot original vs quantized; show error curve; histogram of errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Generate a smooth signal and visualize quantization\n",
    "# x = np.linspace(-1, 1, 1000)\n",
    "# y = np.sin(2 * np.pi * x)  # or try gaussian noise\n",
    "\n",
    "# def uniform_quantize(arr, bits):\n",
    "#     # Symmetric uniform quantization (conceptual)\n",
    "#     # TODO: compute min/max, step size Δ, quantize-dequantize, return y_hat and Δ\n",
    "#     pass\n",
    "\n",
    "# for bits in [16, 8, 4, 2]:\n",
    "#     # TODO: y_hat, delta = uniform_quantize(y, bits)\n",
    "#     # TODO: compute MSE, max error, plot overlays\n",
    "#     # TODO: plot histogram of (y - y_hat)\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ca938",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Module 2 — DNN Primer (CIFAR‑10 + Small CNN + MobileNetV2)\n",
    "\n",
    "**Goal:** Introduce a fast, interpretable DNN to be quantized later.  \n",
    "**Dataset:** CIFAR‑10 (32×32 RGB).  \n",
    "**Models:** \n",
    "- Small CNN (custom) — train a few epochs for ~60‑70% acc (CPU‑friendly).\n",
    "- Pretrained MobileNetV2 — use for PTQ/QAT comparison and realism.\n",
    "\n",
    "### What to do\n",
    "1. Load CIFAR‑10 with basic transforms.\n",
    "2. Define a **small CNN** (`Conv → ReLU → Pool → Conv → ReLU → FC`) < 1M params.\n",
    "3. Train briefly; evaluate test accuracy and latency (CPU).\n",
    "4. Inspect and **visualize weight histograms** (conv layers & FC).\n",
    "5. Print **model size** (saved .pt) and basic metrics.\n",
    "6. Load **MobileNetV2** from `torchvision.models` and run a quick eval on CIFAR‑10 (or subset). Print its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: CIFAR-10 dataset & dataloaders\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# test_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "# train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2)\n",
    "# test_loader = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = train_ds.classes\n",
    "# classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57694666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Define a small CNN model (keep it <1M params; simple and didactic)\n",
    "# class SmallCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # TODO: define conv layers, pooling, and a small classifier head\n",
    "#     def forward(self, x):\n",
    "#         # TODO: forward pass\n",
    "#         pass\n",
    "\n",
    "# model = SmallCNN()\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d452365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Train the small CNN briefly (1-3 epochs) and report accuracy\n",
    "# device = torch.device('cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def train_one_epoch(model, loader):\n",
    "#     # TODO: simple training loop\n",
    "#     pass\n",
    "\n",
    "# def evaluate(model, loader):\n",
    "#     # TODO: compute accuracy; also measure avg forward latency for a few batches\n",
    "#     pass\n",
    "\n",
    "# for epoch in range(1):  # increase to 2-3 if time allows\n",
    "#     # train_one_epoch(model, train_loader)\n",
    "#     # acc, lat_ms = evaluate(model, test_loader)\n",
    "#     # print(f\"Epoch {epoch}: acc={acc:.2f}% latency={lat_ms:.2f} ms\")\n",
    "#     pass\n",
    "\n",
    "# # TODO: Save FP32 model and report file size\n",
    "# # torch.save(model.state_dict(), \"smallcnn_fp32.pt\")\n",
    "# # print(\"SmallCNN FP32 size (MB):\", os.path.getsize(\"smallcnn_fp32.pt\")/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78230f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Visualize weight histograms (conv layers & fc)\n",
    "# with torch.no_grad():\n",
    "#     # for each named parameter, if 'weight' in name: collect and plot histogram\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e687f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Load pretrained MobileNetV2 and adapt to CIFAR-10 (optional: replace classifier head)\n",
    "# mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "# # Optional: replace final classifier for CIFAR-10; or evaluate feature extractor qualitatively\n",
    "# # TODO: quick eval / dummy pass / size reporting\n",
    "# # torch.save(mobilenet.state_dict(), \"mobilenetv2_fp32.pt\")\n",
    "# # print(\"MobileNetV2 FP32 size (MB):\", os.path.getsize(\"mobilenetv2_fp32.pt\")/1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68462514",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Module 3 — Post‑Training Quantization (PTQ)\n",
    "\n",
    "**Goal:** Use PyTorch built‑ins to quantize FP32 models and compare **accuracy**, **latency**, and **model size**.\n",
    "\n",
    "### What to do\n",
    "1. PTQ on the **Small CNN** (static or dynamic quantization depending on layer support).\n",
    "2. PTQ on **MobileNetV2** (dynamic quantization typically).\n",
    "3. Build a comparison table:\n",
    "   - FP32 vs INT8: accuracy (%), latency (ms), size (MB).\n",
    "\n",
    "> Tip: Dynamic quantization targets Linear/LSTM layers; static quant requires calibration on representative data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: PTQ for Small CNN (static or dynamic)\n",
    "# from torch.ao.quantization import quantize_dynamic, get_default_qconfig, prepare, convert\n",
    "\n",
    "# # Option A: Dynamic quantization (works well for Linear layers)\n",
    "# # smallcnn_int8 = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "# # torch.save(smallcnn_int8.state_dict(), \"smallcnn_int8_ptq.pt\")\n",
    "\n",
    "# # Option B: Static quantization (requires qconfig, fuse modules, prepare, calibration, convert)\n",
    "# # model.qconfig = get_default_qconfig('fbgemm')\n",
    "# # fused = torch.ao.quantization.fuse_modules(...)  # TODO if applicable\n",
    "# # prepared = prepare(model, inplace=False)\n",
    "# # # Calibration: run a few batches from train_loader through 'prepared' in eval mode\n",
    "# # # convert\n",
    "# # quantized_smallcnn = convert(prepared)\n",
    "\n",
    "# # TODO: Evaluate metrics (accuracy, latency, size)\n",
    "# # print(\"SmallCNN INT8 PTQ size (MB):\", os.path.getsize(\"smallcnn_int8_ptq.pt\")/1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: PTQ for MobileNetV2 (usually dynamic quantization on Linear layers)\n",
    "# # mobilenet_int8 = quantize_dynamic(mobilenet, {nn.Linear}, dtype=torch.qint8)\n",
    "# # torch.save(mobilenet_int8.state_dict(), \"mobilenetv2_int8_ptq.pt\")\n",
    "# # print(\"MobileNetV2 INT8 PTQ size (MB):\", os.path.getsize(\"mobilenetv2_int8_ptq.pt\")/1e6)\n",
    "\n",
    "# # TODO: quick accuracy and latency check (on CIFAR-10 subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Summarize PTQ results in a small table/dict for display\n",
    "# results_ptq = {\n",
    "#     'Model': ['SmallCNN FP32', 'SmallCNN INT8 PTQ', 'MobileNetV2 FP32', 'MobileNetV2 INT8 PTQ'],\n",
    "#     'Accuracy_%': [None, None, None, None],\n",
    "#     'Latency_ms': [None, None, None, None],\n",
    "#     'Size_MB': [None, None, None, None],\n",
    "# }\n",
    "# results_ptq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befc8ec",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Module 4 — Quantization‑Aware Training (QAT)\n",
    "\n",
    "**Goal:** Show that training with quantization awareness recovers accuracy relative to PTQ.\n",
    "\n",
    "### What to do\n",
    "1. Configure QAT on **Small CNN** using PyTorch QAT APIs.\n",
    "2. Fine‑tune for 1–2 epochs with fake quantization enabled.\n",
    "3. Convert to a quantized model and compare **accuracy**, **latency**, and **size** with FP32 and PTQ.\n",
    "\n",
    "> Tip: Keep QAT minimal for the session; even a small amount of fine‑tuning is illustrative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689da17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: QAT flow (PyTorch built-ins)\n",
    "# from torch.ao.quantization import get_default_qat_qconfig, prepare_qat\n",
    "\n",
    "# # model_fp32_for_qat = SmallCNN()\n",
    "# # model_fp32_for_qat.load_state_dict(torch.load(\"smallcnn_fp32.pt\"))\n",
    "# # model_fp32_for_qat.train()\n",
    "# # model_fp32_for_qat.qconfig = get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# # prepared_qat = prepare_qat(model_fp32_for_qat, inplace=False)\n",
    "# # # Train for a small number of epochs with standard optimizer/loss\n",
    "# # # After fine-tune: convert to quantized\n",
    "# # quantized_qat_model = convert(prepared_qat.eval(), inplace=False)\n",
    "\n",
    "# # TODO: Evaluate metrics and record in a results table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32273f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Summarize QAT results\n",
    "# results_qat = {\n",
    "#     'Model': ['SmallCNN FP32', 'SmallCNN INT8 PTQ', 'SmallCNN INT8 QAT'],\n",
    "#     'Accuracy_%': [None, None, None],\n",
    "#     'Latency_ms': [None, None, None],\n",
    "#     'Size_MB': [None, None, None],\n",
    "# }\n",
    "# results_qat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e09c9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Module 5 — User‑Defined Quantizer (Advanced)\n",
    "\n",
    "**Goal:** Implement linear quantization by hand and make it QAT‑aware with **Straight‑Through Estimator (STE)**.\n",
    "\n",
    "### Concepts to Emphasize\n",
    "- Linear quantization: \\( \\hat{x} = \\mathrm{round}(x / \\Delta) \\cdot \\Delta \\)\n",
    "- Step size: \\( \\Delta = \\frac{\\beta - \\alpha}{2^b - 1} \\) given clipping range \\([\\alpha, \\beta]\\)\n",
    "- **Symmetric vs Asymmetric**, **per‑tensor vs per‑channel**\n",
    "- STE passes gradients as identity during backprop through the discretization\n",
    "\n",
    "### What to do\n",
    "1. Write a pure‑PyTorch function to quantize **weights** given bit‑width **b**, range, and rounding mode.\n",
    "2. Wrap it in an `nn.Module` that applies quantization in `forward()` for **QAT** (with STE).\n",
    "3. Insert into Small CNN and fine‑tune briefly; compare results across bit‑widths.\n",
    "4. Visualize quantized vs original weight histograms and report accuracy/latency/size.\n",
    "\n",
    "> Keep the first pass **per‑tensor symmetric** for clarity, then optionally extend to **per‑channel**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30845d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Manual weight quantizer (linear, symmetric)\n",
    "# def linear_quantize_tensor(x, bits=8, clip_ratio=0.999):\n",
    "#     # TODO: choose alpha/beta from observed range or percentile; compute Δ; quantize/dequantize\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: STE-enabled module for QAT\n",
    "# class STEQuantizer(nn.Module):\n",
    "#     def __init__(self, bits=8):\n",
    "#         super().__init__()\n",
    "#         self.bits = bits\n",
    "#     def forward(self, x):\n",
    "#         # TODO: quantize in forward; identity gradient in backward (via .detach() trick or custom autograd.Function)\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65609811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Integrate STEQuantizer into SmallCNN (e.g., wrapping weights or as fake-quant nodes)\n",
    "# class SmallCNN_QAT(nn.Module):\n",
    "#     def __init__(self, bits=8):\n",
    "#         super().__init__()\n",
    "#         # TODO: define layers; include STEQuantizer modules where appropriate\n",
    "#     def forward(self, x):\n",
    "#         # TODO: apply quantization to weights/activations (start with weights)\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Experiment: train/evaluate with bits in [8, 4, 2]; log accuracy and errors\n",
    "# for b in [8, 4, 2]:\n",
    "#     # model_q = SmallCNN_QAT(bits=b)\n",
    "#     # TODO: brief fine-tuning + evaluation\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796d2d7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Wrap‑Up & Discussion\n",
    "\n",
    "- **PTQ vs QAT:** When is each appropriate? What accuracy trade‑offs did you observe?\n",
    "- **Custom quantization:** How close did manual QAT get to built‑in QAT?\n",
    "- **Bit‑width sensitivity:** Which layers were most sensitive? (Conv vs FC)\n",
    "- **Deployment:** How would these INT8 models map to MCUs/NPUs/FPGAs?\n",
    "\n",
    "> Next steps (beyond this notebook): add **pruning** and **mixed‑precision** experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d83edf",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "*Prepared on:* 2025-10-15 07:54 UTC  \n",
    "*Notes:* Replace TODO blocks with live coding during the seminar. Keep iterations short and show results early.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
