{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ofa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw3mXIy7O4LQ",
        "outputId": "0834107d-1d15-4028-bde8-55129c4238f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ofa\n",
            "  Downloading ofa-0.1.0.post202307202001-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from ofa) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->ofa) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->ofa) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->ofa) (3.0.3)\n",
            "Downloading ofa-0.1.0.post202307202001-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ofa\n",
            "Successfully installed ofa-0.1.0.post202307202001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset # <-- For calibration\n",
        "import random # <-- For calibration subset\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import requests\n",
        "import tarfile  # For .tgz files\n",
        "import shutil   # For deleting a corrupt zip/dir\n",
        "# import pandas as pd # <-- No longer needed\n",
        "\n",
        "try:\n",
        "    from ofa.model_zoo import ofa_net\n",
        "except ImportError as e:\n",
        "    print(\"Error: 'ofa' library not found or a component is missing.\")\n",
        "    print(f\"Import error details: {e}\")\n",
        "    print(\"Please run: pip install ofa\")\n",
        "    sys.exit(1)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLdZPmFcO7hV",
        "outputId": "0202b783-f35a-4868-f125-15ad822ae313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SETUP & DATA LOADING ---\n",
        "\n",
        "# Download ImageNet 1000-class labels (we need this for the mapping)\n",
        "print(\"Downloading ImageNet 1000-class index...\")\n",
        "label_url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "class_idx = json.load(urllib.request.urlopen(label_url))\n",
        "imagenet_labels = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "# Create a WordNet ID -> 1000-class index mapping\n",
        "# e.g., 'n02110185' -> 258 (Samoyed)\n",
        "wordnet_to_1000_idx = {v[0]: int(k) for k, v in class_idx.items()}\n",
        "print(\"ImageNet 1000-class index loaded.\")\n",
        "\n",
        "def download_and_untar_imagenette():\n",
        "    \"\"\"\n",
        "    Downloads and unzips the Imagenette dataset if not already present.\n",
        "    \"\"\"\n",
        "    dataset_dir = \"imagenette2-160\"\n",
        "    tgz_path = f\"{dataset_dir}.tgz\"\n",
        "    url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\"\n",
        "\n",
        "    if os.path.exists(dataset_dir):\n",
        "        print(\"Imagenette directory already exists.\")\n",
        "        return dataset_dir\n",
        "\n",
        "    if not os.path.exists(tgz_path):\n",
        "        print(f\"Downloading Imagenette (150MB) from {url}...\")\n",
        "        print(\"This may take a few minutes...\")\n",
        "        try:\n",
        "            with requests.get(url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(tgz_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            print(\"Download complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading Imagenette: {e}\")\n",
        "            if os.path.exists(tgz_path):\n",
        "                os.remove(tgz_path) # Clean up partial download\n",
        "            sys.exit(1)\n",
        "\n",
        "    print(f\"Un-tarring {tgz_path}...\")\n",
        "    try:\n",
        "        with tarfile.open(tgz_path, \"r:gz\") as tar_ref:\n",
        "            tar_ref.extractall()\n",
        "        print(f\"Successfully un-tarred to {dataset_dir}.\")\n",
        "        # os.remove(tgz_path) # Optional: remove tgz file after extraction\n",
        "    except Exception as e:\n",
        "        print(f\"Error un-tarring file: {e}. Please delete it and re-run.\")\n",
        "        shutil.rmtree(dataset_dir, ignore_errors=True) # Clean up bad extraction\n",
        "        os.remove(tgz_path)\n",
        "        sys.exit(1)\n",
        "\n",
        "    return dataset_dir\n",
        "\n",
        "# Standard ImageNet transforms\n",
        "def get_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "# --- NEW DATA LOADING FUNCTION ---\n",
        "def load_imagenette_val_set(dataset_dir, wordnet_to_1000_idx_map, max_images=100):\n",
        "    \"\"\"\n",
        "    Loads a subset of the Imagenette validation set using ImageFolder\n",
        "    and maps its 0-9 classes to the correct 1000-class ImageNet index.\n",
        "    \"\"\"\n",
        "    print(f\"Loading {max_images} images from Imagenette validation set...\")\n",
        "\n",
        "    transform = get_transforms()\n",
        "\n",
        "    val_dir = os.path.join(dataset_dir, 'val')\n",
        "    if not os.path.exists(val_dir):\n",
        "        print(f\"Error: Validation directory not found at {val_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Load the dataset using ImageFolder\n",
        "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "    # Create the mapping from ImageFolder's 0-9 index to the 1000-class index\n",
        "    # val_dataset.class_to_idx is {'n01440764': 0, 'n02102040': 1, ...}\n",
        "    imgfolder_idx_to_1000_idx = {}\n",
        "    for wordnet_id, imgfolder_idx in val_dataset.class_to_idx.items():\n",
        "        if wordnet_id in wordnet_to_1000_idx_map:\n",
        "            idx_1000 = wordnet_to_1000_idx_map[wordnet_id]\n",
        "            imgfolder_idx_to_1000_idx[imgfolder_idx] = idx_1000\n",
        "        else:\n",
        "            # This class isn't in the 1000-class set (shouldn't happen for Imagenette)\n",
        "            imgfolder_idx_to_1000_idx[imgfolder_idx] = -1 # Flag to skip\n",
        "\n",
        "    print(\"ImageFolder mapping to 1000-class index created.\")\n",
        "\n",
        "    val_set = []\n",
        "    # Load a subset of images\n",
        "    for i in range(len(val_dataset)):\n",
        "        if i >= max_images:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Get the pre-processed image and its 0-9 index\n",
        "            input_tensor, imgfolder_idx = val_dataset[i]\n",
        "\n",
        "            # Translate to the 1000-class index\n",
        "            true_label_1000_idx = imgfolder_idx_to_1000_idx.get(imgfolder_idx, -1)\n",
        "\n",
        "            if true_label_1000_idx != -1:\n",
        "                true_label_name = imagenet_labels[true_label_1000_idx]\n",
        "                # Add the batch dimension\n",
        "                val_set.append((input_tensor.unsqueeze(0).to(DEVICE), true_label_1000_idx, true_label_name))\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not process image at index {i}. Skipping. Error: {e}\")\n",
        "\n",
        "    print(f\"Val set ready with {len(val_set)} images.\")\n",
        "    return val_set\n",
        "\n",
        "# --- NEW: Function to load training data for BN calibration ---\n",
        "def load_imagenette_train_loader(dataset_dir, batch_size=64, num_samples=2000):\n",
        "    \"\"\"\n",
        "    Loads a subset of the Imagenette training set for BN calibration.\n",
        "    \"\"\"\n",
        "    print(f\"Loading calibration data from Imagenette train set...\")\n",
        "    transform = get_transforms()\n",
        "    train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "    if not os.path.exists(train_dir):\n",
        "        print(f\"Error: Training directory not found at {train_dir}\")\n",
        "        return None\n",
        "\n",
        "    train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "    # Create a random subset for faster calibration\n",
        "    num_total_samples = len(train_dataset)\n",
        "    subset_indices = random.sample(range(num_total_samples), min(num_samples, num_total_samples))\n",
        "    calibration_subset = Subset(train_dataset, subset_indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        calibration_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "    print(f\"Calibration data loader ready with {len(calibration_subset)} images.\")\n",
        "    return train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP8VPKlNPgJV",
        "outputId": "292ca4eb-7ebd-4cb2-8b34-6db2df26419a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ImageNet 1000-class index...\n",
            "ImageNet 1000-class index loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. EVALUATION FUNCTIONS ---\n",
        "\n",
        "# --- NEW: Function to calibrate Batch Norm stats ---\n",
        "def calibrate_model_bn(model, train_loader, model_name):\n",
        "    \"\"\"\n",
        "    Recalculates the running mean and variance for the Batch Norm layers\n",
        "    of a sampled sub-network.\n",
        "    \"\"\"\n",
        "    print(f\"Calibrating Batch Norm for {model_name}...\")\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Set model to train() mode to update BN stats\n",
        "    model.train()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, _) in enumerate(train_loader):\n",
        "            images = images.to(DEVICE)\n",
        "            # Just run the forward pass to update BN stats\n",
        "            model(images)\n",
        "            if i > 100: # Calibrate on ~100 batches\n",
        "                break\n",
        "\n",
        "    # Set model back to eval() mode for inference\n",
        "    model.eval()\n",
        "    print(f\"Calibration complete for {model_name}.\")\n",
        "\n",
        "\n",
        "# Approximate FLOPs calculation\n",
        "def get_flops(model, input_shape=(1, 3, 224, 224)):\n",
        "    model.eval()\n",
        "    input_tensor = torch.randn(input_shape).to(DEVICE)\n",
        "    flops = 0\n",
        "    def hook(module, input, output):\n",
        "        nonlocal flops\n",
        "        # This is a rough approximation (counts MACs)\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            # output H * W * C_out * (C_in * K_h * K_w)\n",
        "            flops += int(output.nelement() * module.in_channels * module.kernel_size[0] * module.kernel_size[1] / module.groups)\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "            # output N * C_out * (C_in)\n",
        "            flops += int(output.nelement() * module.in_features)\n",
        "\n",
        "    handles = [m.register_forward_hook(hook) for m in model.modules()]\n",
        "    with torch.no_grad():\n",
        "        model(input_tensor)\n",
        "    for h in handles:\n",
        "        h.remove()\n",
        "    return f\"{flops / 1e9:.2f} G\" # Return Giga-FLOPs\n",
        "\n",
        "# Updated evaluation with FLOPs and more runs\n",
        "def run_evaluation(model, val_set, model_name, n_runs_per_image=1):\n",
        "    model.eval() # Ensure model is in eval mode for evaluation\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "\n",
        "    # Run FLOPs calculation once\n",
        "    try:\n",
        "        flops = get_flops(model)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not calculate FLOPs for {model_name}. Error: {e}\")\n",
        "        flops = \"N/A\"\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_images = len(val_set)\n",
        "    total_latency_ms = 0\n",
        "\n",
        "    print(f\"\\n--- Evaluating {model_name} (avg over {n_runs_per_image} runs per image) ---\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Extended warm-up\n",
        "        if val_set:\n",
        "            print(\"Warming up...\")\n",
        "            for _ in range(3):\n",
        "                _ = model(val_set[0][0])\n",
        "\n",
        "        print(\"Starting evaluation...\")\n",
        "        for i, (input_tensor, true_label_idx, true_label_name) in enumerate(val_set):\n",
        "            current_image_time_ms = 0\n",
        "            output = None\n",
        "            for _ in range(n_runs_per_image):\n",
        "                start_time = time.time()\n",
        "                # input_tensor is already pre-processed and on the correct device\n",
        "                output = model(input_tensor)\n",
        "                end_time = time.time()\n",
        "                current_image_time_ms += (end_time - start_time) * 1000\n",
        "\n",
        "            avg_time_for_this_image = current_image_time_ms / n_runs_per_image\n",
        "            total_latency_ms += avg_time_for_this_image\n",
        "\n",
        "            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "            top1_prob, top1_catid = torch.topk(probabilities, 1)\n",
        "\n",
        "            prediction_idx = top1_catid.item()\n",
        "            prediction_name = imagenet_labels[prediction_idx]\n",
        "\n",
        "            # Simple progress print\n",
        "            if (i+1) % 20 == 0:\n",
        "                print(f\"  Processed {i+1}/{total_images} images...\")\n",
        "\n",
        "            if prediction_idx == true_label_idx:\n",
        "                correct_predictions += 1\n",
        "                # print(f\" [✓] Correct: '{true_label_name}' (Latency: {avg_time_for_this_image:.2f} ms)\")\n",
        "            # else:\n",
        "                # print(f\" [X] WRONG: Predicted '{prediction_name}', True label was '{true_label_name}' (Latency: {avg_time_for_this_image:.2f} ms)\")\n",
        "\n",
        "    if total_images == 0:\n",
        "        print(\"Error: No images were loaded in the validation set. Cannot evaluate.\")\n",
        "        return {\"name\": model_name, \"params_m\": f\"{params_m:.2f} M\", \"flops\": flops, \"accuracy\": \"N/A\", \"latency\": \"N/A\"}\n",
        "\n",
        "    accuracy = (correct_predictions / total_images) * 100\n",
        "    avg_latency = total_latency_ms / total_images\n",
        "\n",
        "    print(f\"\\nResult: {correct_predictions} / {total_images} correct ({accuracy:.1f}%)\")\n",
        "    print(f\"Efficiency (Size): {params_m:.2f} Million Parameters\")\n",
        "    print(f\"FLOPs (Approx): {flops}\")\n",
        "    print(f\"Avg Latency per Image: {avg_latency:.2f} ms ({'(GPU)' if DEVICE == 'cuda' else '(CPU)'})\")\n",
        "\n",
        "    return {\"name\": model_name, \"params_m\": f\"{params_m:.2f} M\", \"flops\": flops, \"accuracy\": f\"{accuracy:.1f}%\", \"latency\": f\"{avg_latency:.2f} ms\"}\n"
      ],
      "metadata": {
        "id": "jb7SNQKhbUqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. MAIN EXECUTION ---\n",
        "\n",
        "# Prepare Imagenette\n",
        "dataset_dir = download_and_untar_imagenette()\n",
        "# We pass the wordnet_to_1000_idx map to the loading function\n",
        "val_set = load_imagenette_val_set(dataset_dir, wordnet_to_1000_idx, max_images=100)\n",
        "# --- NEW: Load calibration data ---\n",
        "train_loader = load_imagenette_train_loader(dataset_dir)\n",
        "\n",
        "\n",
        "if not val_set or not train_loader:\n",
        "    print(\"Error: Validation or Training set is empty. Cannot proceed.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Load supernet\n",
        "print(\"\\nLoading OFA MobileNetV3 super-network...\")\n",
        "ofa_super_network = ofa_net('ofa_mbv3_d234_e346_k357_w1.2', pretrained=True)\n",
        "print(\"Super-network loaded.\")\n",
        "\n",
        "# Sample and evaluate subnetworks\n",
        "results = []\n",
        "\n",
        "# Tiny spec (unchanged)\n",
        "tiny_spec = {'d': [2]*5, 'e': [3]*20, 'k': [3]*20}\n",
        "print(\"\\nSampling 'Tiny Model'...\")\n",
        "ofa_super_network.set_active_subnet(ks=tiny_spec['k'], e=tiny_spec['e'], d=tiny_spec['d'])\n",
        "tiny_model = ofa_super_network.get_active_subnet(preserve_weight=True)\n",
        "# --- NEW: Calibrate BN ---\n",
        "calibrate_model_bn(tiny_model, train_loader, \"Tiny Model\")\n",
        "results.append(run_evaluation(tiny_model, val_set, \"Tiny Model (Target: Microcontroller)\"))\n",
        "\n",
        "# New: Medium spec\n",
        "medium_spec = {'d': [3]*5, 'e': [4]*10 + [5]*10, 'k': [5]*20}\n",
        "print(\"\\nSampling 'Medium Model'...\")\n",
        "ofa_super_network.set_active_subnet(ks=medium_spec['k'], e=medium_spec['e'], d=medium_spec['d'])\n",
        "medium_model = ofa_super_network.get_active_subnet(preserve_weight=True)\n",
        "# --- NEW: Calibrate BN ---\n",
        "calibrate_model_bn(medium_model, train_loader, \"Medium Model\")\n",
        "results.append(run_evaluation(medium_model, val_set, \"Medium Model (Target: Edge Device)\"))\n",
        "\n",
        "# Large spec (unchanged)\n",
        "large_spec = {'d': [4]*5, 'e': [6]*20, 'k': [7]*20}\n",
        "print(\"\\nSampling 'Large Model'...\")\n",
        "ofa_super_network.set_active_subnet(ks=large_spec['k'], e=large_spec['e'], d=large_spec['d'])\n",
        "large_model = ofa_super_network.get_active_subnet(preserve_weight=True)\n",
        "# --- NEW: Calibrate BN ---\n",
        "calibrate_model_bn(large_model, train_loader, \"Large Model\")\n",
        "results.append(run_evaluation(large_model, val_set, \"Large Model (Target: Smartphone)\"))\n",
        "\n",
        "# Final comparison with FLOPs\n",
        "print(\"\\n\\n--- Demo Summary ---\")\n",
        "print(f\"Evaluated on a {len(val_set)}-image subset of the Imagenette validation set.\")\n",
        "print(\"All models sampled from the SAME pre-trained super-network, with NO re-training.\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"| {'Model':<30} | {'Params (Size)':<15} | {'FLOPs (Approx)':<15} | {'Avg Latency':<15} | {'Accuracy':<15} |\")\n",
        "print(f\"| {'-'*30} | {'-'*15} | {'-'*15} | {'-'*15} | {'-'*15} |\")\n",
        "for res in results:\n",
        "    print(f\"| {res['name']:<30} | {res['params_m']:<15} | {res['flops']:<15} | {res['latency']:<15} | {res['accuracy']:<15} |\")\n",
        "print(\"-\" * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP3u6n8bQDsv",
        "outputId": "7fdd801b-1ec8-4b52-e4a4-616899906613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Imagenette (150MB) from https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz...\n",
            "This may take a few minutes...\n",
            "Download complete.\n",
            "Un-tarring imagenette2-160.tgz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1704095814.py:44: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar_ref.extractall()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully un-tarred to imagenette2-160.\n",
            "Loading 100 images from Imagenette validation set...\n",
            "ImageFolder mapping to 1000-class index created.\n",
            "Val set ready with 100 images.\n",
            "Loading calibration data from Imagenette train set...\n",
            "Calibration data loader ready with 2000 images.\n",
            "\n",
            "Loading OFA MobileNetV3 super-network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://raw.githubusercontent.com/han-cai/files/master/ofa/ofa_nets/ofa_mbv3_d234_e346_k357_w1.2\" to .torch/ofa_nets/ofa_mbv3_d234_e346_k357_w1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Super-network loaded.\n",
            "\n",
            "Sampling 'Tiny Model'...\n",
            "Calibrating Batch Norm for Tiny Model...\n",
            "Calibration complete for Tiny Model.\n",
            "\n",
            "--- Evaluating Tiny Model (Target: Microcontroller) (avg over 1 runs per image) ---\n",
            "Warming up...\n",
            "Starting evaluation...\n",
            "  Processed 20/100 images...\n",
            "  Processed 40/100 images...\n",
            "  Processed 60/100 images...\n",
            "  Processed 80/100 images...\n",
            "  Processed 100/100 images...\n",
            "\n",
            "Result: 75 / 100 correct (75.0%)\n",
            "Efficiency (Size): 4.60 Million Parameters\n",
            "FLOPs (Approx): 0.19 G\n",
            "Avg Latency per Image: 5.60 ms ((GPU))\n",
            "\n",
            "Sampling 'Medium Model'...\n",
            "Calibrating Batch Norm for Medium Model...\n",
            "Calibration complete for Medium Model.\n",
            "\n",
            "--- Evaluating Medium Model (Target: Edge Device) (avg over 1 runs per image) ---\n",
            "Warming up...\n",
            "Starting evaluation...\n",
            "  Processed 20/100 images...\n",
            "  Processed 40/100 images...\n",
            "  Processed 60/100 images...\n",
            "  Processed 80/100 images...\n",
            "  Processed 100/100 images...\n",
            "\n",
            "Result: 86 / 100 correct (86.0%)\n",
            "Efficiency (Size): 7.20 Million Parameters\n",
            "FLOPs (Approx): 0.42 G\n",
            "Avg Latency per Image: 8.06 ms ((GPU))\n",
            "\n",
            "Sampling 'Large Model'...\n",
            "Calibrating Batch Norm for Large Model...\n",
            "Calibration complete for Large Model.\n",
            "\n",
            "--- Evaluating Large Model (Target: Smartphone) (avg over 1 runs per image) ---\n",
            "Warming up...\n",
            "Starting evaluation...\n",
            "  Processed 20/100 images...\n",
            "  Processed 40/100 images...\n",
            "  Processed 60/100 images...\n",
            "  Processed 80/100 images...\n",
            "  Processed 100/100 images...\n",
            "\n",
            "Result: 89 / 100 correct (89.0%)\n",
            "Efficiency (Size): 10.70 Million Parameters\n",
            "FLOPs (Approx): 0.84 G\n",
            "Avg Latency per Image: 10.37 ms ((GPU))\n",
            "\n",
            "\n",
            "--- Demo Summary ---\n",
            "Evaluated on a 100-image subset of the Imagenette validation set.\n",
            "All models sampled from the SAME pre-trained super-network, with NO re-training.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "| Model                          | Params (Size)   | FLOPs (Approx)  | Avg Latency     | Accuracy        |\n",
            "| ------------------------------ | --------------- | --------------- | --------------- | --------------- |\n",
            "| Tiny Model (Target: Microcontroller) | 4.60 M          | 0.19 G          | 5.60 ms         | 75.0%           |\n",
            "| Medium Model (Target: Edge Device) | 7.20 M          | 0.42 G          | 8.06 ms         | 86.0%           |\n",
            "| Large Model (Target: Smartphone) | 10.70 M         | 0.84 G          | 10.37 ms        | 89.0%           |\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}